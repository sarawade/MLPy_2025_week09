{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bf70df4-9fbe-4a43-8c58-658ae1a4b5d1",
   "metadata": {},
   "source": [
    "# Introduction to Neural network (Keras + MNIST)\n",
    "\n",
    "### Aims\n",
    "\n",
    "The main concepts covered in this notebook are: \n",
    "\n",
    ">* getting familiar with basic keras\n",
    ">* input-output with keras\n",
    ">* construciton of neural network models with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f92e8c-4b27-4f93-a43f-784066896a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d79628a-04b6-4079-b2a9-7b3a357900a9",
   "metadata": {},
   "source": [
    "The following code boxes will allow you to visualise your model training. Scroll back up to take a look once you get to a \"model.fit\" statement! (You'll need to refresh the dashboard with the refresh button on the top right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a72376c-6857-4bc2-99f9-5fd0f129a230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, datetime\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ee53ef-b10a-41e8-80d8-4457d7c5fda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --port=5036 --logdir $logdir\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(logdir, histogram_freq=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd8f17a-0921-4b9e-9012-b0439e33d4e1",
   "metadata": {},
   "source": [
    "# Part 1: Sythetic Data\n",
    "\n",
    "In the first part of this workshop we will work with a \"clean\" dataset, generating data from purely deterministic functions.\n",
    "\n",
    "First, generate data according to \n",
    "\\begin{equation}\n",
    "  x_2 = \\cos(x_1),\n",
    "\\end{equation}\n",
    "with $x_1 \\in [-\\pi, \\pi]$. This data will be labelled as belonging to class $y=0$.\n",
    "\n",
    "For data in class $y=1$, generate \n",
    "\\begin{equation}\n",
    "  x_2 = a + \\cos(x_1),\n",
    "\\end{equation}\n",
    "where $a=1$ for now.\n",
    "\n",
    "You should generate ~2000 samples for $x_1$ (uniform distribution over $x_1\\sim U[-\\pi,\\pi]$).\n",
    "\n",
    "For use in keras, it helps to build numpy arrays of following shape \n",
    "$$X.\\text{shape}=(N, D)$$\n",
    "with a corresponding set of labels\n",
    "$$y.\\text{shape}=(N,)$$\n",
    "where $N$ is the number of samples and $D$ the number of features ($D=2$ here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202978ed-6db2-44ba-ba2f-0f63e714e929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faad502-cae3-4993-881c-ac3193a1a4bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c293519f-5cfc-40cc-b2aa-509d3f09e317",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 1 (CORE): Building a Baseline Model\n",
    "\n",
    "a) Build a logistic regression model in keras.  The model should consist of an input layer and a fully-connected output layer. No hideen layer for now.\n",
    "See lecture notes for details of how to create these objects, or ask your tutors. \n",
    "\n",
    "b) Compile the model. At this stage you need to select a loss function (specified via the \"loss\" keyword) and an optimizer. Any optimizer will do -- you could use one of the \"exciting\" ones, e.g. Adam.  \n",
    "\n",
    "c. Train the model with model.fit. Pass the keyword argument \n",
    "\n",
    "```\n",
    "# callbacks=[tensorboard_callback]\n",
    "```\n",
    "\n",
    "to visualise above. \n",
    "\n",
    "You might also want to split the dataset into a training and validation component via \n",
    "\n",
    "\n",
    "```\n",
    "# validation_split=0.X\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe735aa-9443-445c-b5cf-3037f07a25eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acadd58a-c85c-46ad-89af-5eeaad487a47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a41f622-6430-405b-9deb-9fc49e24874c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b468154d-37bd-45e7-a0d7-b5014b626df7",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 2 (CORE): Testing your model\n",
    "\n",
    "Generate \"test\" data uniformly over $x_1\\in[-\\pi, \\pi]$ and $x_2\\in[-1, 1+a]$. Use your trained model to predict the $y$ labels for this data and visualise the results. Overlay the original curves on the output -- is the result what you expect, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418f3c1c-e2c1-4a6b-9fd4-5d9a4f285af1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d0c5ca-ccdf-4ee5-9c7c-b23f7a52edb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e66b62c-7e1d-49fd-882f-7fac903fe6f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bbbb3b1-3a8e-47ff-b36e-ea15654eaf6d",
   "metadata": {},
   "source": [
    "The preditcion is clearly wrong. We xan assume that the absence of hidden layers has left the system with too little complexity to recreate the true distribution. In toher word, the poool of functions in the form $G(x) = \\sum_n \\sigma(\\alpha_n x_n + \\theta_n)$ is too small to correctly approximate $f(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07765926-d730-496a-9265-94686ced275f",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 3 (CORE): Building a Baseline Model\n",
    "\n",
    "Now create a new model by adding a fully-connected hidden layer with 2 neurons between your input and output above. \n",
    "\n",
    "Train the new model and visualise the same test data from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17bbdf1-1c52-4194-812a-ec94fb06c91e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d545e8-9638-4b5c-9ed3-da62b2778dee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999ce538-e4f7-4154-861f-8c485a0b9c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e697444e-7ca1-491f-8cf4-08d24efd38e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4408853-9642-4da2-a107-179be83d38a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66ddf654-8233-44ad-a69f-b1f2928cd9c7",
   "metadata": {},
   "source": [
    "# Part 2: MNIST Dataset and Study of the Hidden Layers\n",
    "\n",
    "### Loading,exploring, and preparing the data\n",
    "\n",
    "For the second part, we are going to use the MNIST dataset, partly because you are already familiar with it, and partly because it comes with tensorflow, the most wideely used library for neural networks in python.\n",
    "\n",
    "a. Load the MNIST dataset using `keras.datasets.mnist.load_data()` and split it into train and test.\n",
    "\n",
    "b. Print the shapes of the training and testing data and labels.\n",
    "\n",
    "c. Display the first image in the training set and its label.\n",
    "\n",
    "d. Normalize the pixel values of the training and testing data to be between 0 and 1.\n",
    "\n",
    "e. Convert the labels to one-hot encoded vectors using `keras.utils.to_categorical()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e551d30b-456e-4c51-8247-4dc19f53c4f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f084bff-43e9-4d6a-a47f-8e8cec70aabd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e44b7d3-66c4-474a-ac74-76fae9d9d5c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00b5477-1e2d-4b03-b413-f1e0519c95c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1ca25cb-8cbf-4ea5-a8b4-c6ca2521999e",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 4 (CORE): Building a Baseline Model\n",
    "\n",
    "a. Build a sequential neural network model with one hidden layer of 128 neurons and an output layer with 10 neurons. Let's try using different activation fucntions (there is no real need to do this here, except learn how to implement it in keras). For example, use ReLU activation for the hidden layer and softmax for the output layer.\n",
    "\n",
    "b. Compile the model using the Adam optimizer, categorical crossentropy loss, and accuracy metric You can do this using the parameters\n",
    "\n",
    "'''\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "'''\n",
    "\n",
    "c. Train the model for 10 epochs with a batch size of 128 and a 10% validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b9a8dd-ee75-4ec7-96b2-a5f15051e0d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c7b8446-6771-4878-96a3-2eee3a6febcf",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 5 (CORE): Exploring Network Depth\n",
    "\n",
    "a. Evaluate the model on the test set and print the test loss and accuracy.\n",
    "\n",
    "b. Plot the training and validation loss curves.\n",
    "\n",
    "c. Generate and visualize a confusion matrix for the test set predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20c9965-cf50-4da7-9551-5b52243af705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ee8b713-8d13-48f0-9275-24d6ac68b711",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 6 (CORE): Exploring Network Depth\n",
    "\n",
    "a. Build a deeper network with two hidden layers, each with 128 neurons and ReLU activation, and an output layer with 10 neurons and softmax activation.\n",
    "\n",
    "b. Compile and train the model as in Exercise 1.\n",
    "\n",
    "c. Evaluate the model on the test set and compare the results with the single-layer model.\n",
    "\n",
    "d. Plot the training and validation loss curves for the deep network.\n",
    "\n",
    "e. Generate and visualize a confusion matrix for the deep network's test set predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dcf9f7-000c-437d-b041-8de39a50af05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1c19ea6-4424-44a4-b695-5441567b0234",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 3 (CORE): When to Stop Training?\n",
    "\n",
    "Compare the training and validation loss curves for `model` (single hidden layer) and `model_deep` (two hidden layers).\n",
    "\n",
    "a. In which scenario do you observe signs of overfitting? Explain your reasoning.\n",
    "\n",
    "b. Based on these graphs, suggest a stopping criterion for training to prevent overfitting.\n",
    "\n",
    "c. How does the depth of the network influence the point at which overfitting begins?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43238552-e9f8-451d-ada2-1504347e82de",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dec02318-2aa1-4a1a-b564-b6467a664988",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 4 (CORE): Going Deep\n",
    "\n",
    "Let's now validate the results in the previous question by increasing the number of hidden layers. We hope to see that the trends we observed when going from one to two hidden layers will be even more pronounced.\n",
    "\n",
    "a. Build a neural network with 10 hidden layers, each with 128 neurons and ReLU activation, and an output layer with 10 neurons and softmax activation.\n",
    "\n",
    "b. Compile and train the model for 20 epochs with a batch size of 128 and a 10% validation split.\n",
    "\n",
    "c. Evaluate the model on the test set and plot the training and validation loss curves.\n",
    "\n",
    "d. Discuss any challenges encountered during training and potential solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca04437-65a5-49fb-8ed6-8e94c56f60a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ce33793-a66b-46fe-b784-610cd33b74b4",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise  (EXTRA): Regularisation Techniques (10 Layers Deep)\n",
    "\n",
    "We have briefly touched on regularisation on Monday, which describes the process of removing complexity from an overfitting network. Here, let's try to implement dropout regularization, which is a technique that randomly ignores (\"drops out\") some layers when the network is overfitting. Look up the technique implementation before having a go. In keras, this is implemented using the \"Dropout\" method for the dropout layers, which accepts a parameter $p$ between 0 and 1, and its effect is to randomly set input units for that layer to 0 with probability $p$ at each step during training time.\n",
    "\n",
    "a. Implement dropout regularization in the 10-layer deep network after each hidden layer with a dropout rate of $p=0.2$.\n",
    "\n",
    "b. Train the regularized model for 20 epochs and compare the training and validation loss curves with the original 10-layer deep model.\n",
    "\n",
    "c. Discuss the impact of dropout regularization on the deep network's performance and generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ab6cac-b6d5-4cf1-b165-944bf3751834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b29fe675-7cbb-44af-b148-994885ffc6df",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7e8280-7a97-46ff-bb98-e62747f9b25e",
   "metadata": {},
   "source": [
    "# Competing the Worksheet\n",
    "\n",
    "At this point you have hopefully been able to complete all the CORE exercises and attempted the EXTRA ones. Now \n",
    "is a good time to check the reproducibility of this document by restarting the notebook's\n",
    "kernel and rerunning all cells in order.\n",
    "\n",
    "Before generating the PDF, please go to Edit -> Edit Notebook Metadata and change 'Student 1' and 'Student 2' in the **name** attribute to include your name. If you are unable to edit the Notebook Metadata, please add a Markdown cell at the top of the notebook with your name(s).\n",
    "\n",
    "Once that is done and you are happy with everything, you can then run the following cell \n",
    "to generate your PDF. Once generated, please submit this PDF on Learn page by 16:00 PM on the Friday of the week the workshop was given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f1f33f-b185-4ded-8b28-8310ffb99bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to pdf mlp_week09_workshop.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d3d9a3-98a5-49b0-9791-c5fa224210d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
